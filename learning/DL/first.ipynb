{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e126b24a",
   "metadata": {},
   "source": [
    "# Data Preprocessing Pipeline ‚Äî Complete Step-by-Step Build\n",
    "\n",
    "I'll guide you through building this **production-grade pipeline** in your `/Users/ankitkumar/Desktop/all_code_of_collage/learning/datapipline` folder.\n",
    "\n",
    "---\n",
    "\n",
    "## üß± STEP 0 ‚Äî Create Project Structure\n",
    "\n",
    "Open Terminal and run:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8bf9d5",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "cd /Users/ankitkumar/Desktop/all_code_of_collage/learning/datapipline\n",
    "mkdir -p src/{ingestion,validation,cleaning,transformation,features,orchestration}\n",
    "mkdir -p configs data/{raw,processed,artifacts} logs reports\n",
    "touch requirements.txt README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e0887b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Your structure is now:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b6b6d",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "datapipline/\n",
    "‚îú‚îÄ‚îÄ src/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ ingestion/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ validation/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ cleaning/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ transformation/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ features/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ orchestration/\n",
    "‚îú‚îÄ‚îÄ configs/\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ raw/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ processed/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ artifacts/\n",
    "‚îú‚îÄ‚îÄ logs/\n",
    "‚îú‚îÄ‚îÄ reports/\n",
    "‚îú‚îÄ‚îÄ main.py\n",
    "‚îî‚îÄ‚îÄ requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0174f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üß± STEP 1 ‚Äî Add Real CSV Data\n",
    "\n",
    "Create file:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1b3112",
   "metadata": {
    "vscode": {
     "languageId": "csv"
    }
   },
   "outputs": [],
   "source": [
    "user_id,signup_date,last_login,age,gender,city,device,session_count,avg_session_duration,transactions,total_spent,is_active\n",
    "U1001,2023-01-15,2026-02-01,21,male,Bangalore,Android,145,18.4,12,4599.50,true\n",
    "U1002,2023-02-03,2026-01-28,34,female,Mumbai,iOS,89,12.7,7,2899.00,true\n",
    "U1003,2022-11-21,2025-12-12,29,male,Delhi,Web,34,6.1,1,499.00,false\n",
    "U1004,2023-05-18,2026-02-02,,female,Chennai,Android,203,21.9,18,7999.99,true\n",
    "U1005,2024-01-09,2026-01-30,41,other,Kolkata,iOS,12,4.2,0,0.00,false\n",
    "U1006,2023-08-27,2026-02-02,25,male,Bangalore,Android,301,33.5,26,12999.00,true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7492b66f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üß± STEP 2 ‚Äî Define Schema (Configuration Contract)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a61fb3",
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "columns:\n",
    "  user_id:\n",
    "    type: string\n",
    "    nullable: false\n",
    "\n",
    "  signup_date:\n",
    "    type: date\n",
    "    nullable: false\n",
    "\n",
    "  last_login:\n",
    "    type: date\n",
    "    nullable: false\n",
    "\n",
    "  age:\n",
    "    type: int\n",
    "    nullable: true\n",
    "    min: 0\n",
    "    max: 120\n",
    "\n",
    "  gender:\n",
    "    type: category\n",
    "    allowed: [male, female, other]\n",
    "\n",
    "  city:\n",
    "    type: string\n",
    "    nullable: false\n",
    "\n",
    "  device:\n",
    "    type: category\n",
    "    allowed: [Android, iOS, Web]\n",
    "\n",
    "  session_count:\n",
    "    type: int\n",
    "    min: 0\n",
    "\n",
    "  avg_session_duration:\n",
    "    type: float\n",
    "    min: 0\n",
    "\n",
    "  transactions:\n",
    "    type: int\n",
    "    min: 0\n",
    "\n",
    "  total_spent:\n",
    "    type: float\n",
    "    min: 0\n",
    "\n",
    "  is_active:\n",
    "    type: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b7fff",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üß± STEP 3 ‚Äî Pipeline Configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5519d5",
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "steps:\n",
    "  ingestion: true\n",
    "  validation: true\n",
    "  cleaning: true\n",
    "  transformation: false\n",
    "  feature_engineering: false\n",
    "\n",
    "cleaning:\n",
    "  missing_values:\n",
    "    age: median\n",
    "  remove_duplicates: true\n",
    "\n",
    "output:\n",
    "  format: csv\n",
    "  save_report: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa894119",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üß± STEP 4 ‚Äî Install Dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffa90a6",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "pandas>=1.3.0\n",
    "numpy>=1.21.0\n",
    "pyyaml>=5.4.0\n",
    "loguru>=0.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd8b559",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Run in Terminal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4db20",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc6bb1a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üß± STEP 5 ‚Äî Data Ingestion Module\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f63e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from loguru import logger\n",
    "\n",
    "def load_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV file with validation.\"\"\"\n",
    "    logger.info(f\"Loading data from: {file_path}\")\n",
    "    \n",
    "    if not file_path.endswith(\".csv\"):\n",
    "        logger.error(\"Only CSV files are supported\")\n",
    "        raise ValueError(\"Only CSV files are supported\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    if df.empty:\n",
    "        logger.error(\"Input dataset is empty\")\n",
    "        raise ValueError(\"Input dataset is empty\")\n",
    "\n",
    "    logger.info(f\"Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9842dd5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üß± STEP 6 ‚Äî Data Validation Engine (CORE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f736aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "from loguru import logger\n",
    "\n",
    "def validate_schema(df: pd.DataFrame, schema_path: str) -> dict:\n",
    "    \"\"\"Validate DataFrame against schema rules.\"\"\"\n",
    "    logger.info(f\"Validating against schema: {schema_path}\")\n",
    "    \n",
    "    with open(schema_path, \"r\") as f:\n",
    "        schema = yaml.safe_load(f)\n",
    "\n",
    "    report = {\"errors\": [], \"warnings\": [], \"passed\": True}\n",
    "\n",
    "    for column, rules in schema[\"columns\"].items():\n",
    "        if column not in df.columns:\n",
    "            report[\"errors\"].append(f\"‚ùå Missing column: {column}\")\n",
    "            report[\"passed\"] = False\n",
    "            continue\n",
    "\n",
    "        # Check nullable constraint\n",
    "        if not rules.get(\"nullable\", True):\n",
    "            null_count = df[column].isnull().sum()\n",
    "            if null_count > 0:\n",
    "                report[\"errors\"].append(f\"‚ùå {null_count} null values in non-nullable column: {column}\")\n",
    "                report[\"passed\"] = False\n",
    "\n",
    "        # Check allowed values\n",
    "        if \"allowed\" in rules:\n",
    "            invalid = df[~df[column].isin(rules[\"allowed\"]) & df[column].notna()]\n",
    "            if not invalid.empty:\n",
    "                report[\"warnings\"].append(f\"‚ö†Ô∏è  {len(invalid)} invalid values in column: {column}\")\n",
    "\n",
    "        # Check min constraint\n",
    "        if \"min\" in rules:\n",
    "            below_min = (df[column].dropna() < rules[\"min\"]).sum()\n",
    "            if below_min > 0:\n",
    "                report[\"warnings\"].append(f\"‚ö†Ô∏è  {below_min} values below min ({rules['min']}) in: {column}\")\n",
    "\n",
    "        # Check max constraint\n",
    "        if \"max\" in rules:\n",
    "            above_max = (df[column].dropna() > rules[\"max\"]).sum()\n",
    "            if above_max > 0:\n",
    "                report[\"warnings\"].append(f\"‚ö†Ô∏è  {above_max} values above max ({rules['max']}) in: {column}\")\n",
    "\n",
    "    logger.info(f\"Validation complete: {len(report['errors'])} errors, {len(report['warnings'])} warnings\")\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27399a5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üß± STEP 7 ‚Äî Data Cleaning Engine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f7c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from loguru import logger\n",
    "\n",
    "def clean_data(df: pd.DataFrame, cleaning_config: dict) -> pd.DataFrame:\n",
    "    \"\"\"Apply cleaning transformations based on config.\"\"\"\n",
    "    logger.info(\"Starting data cleaning...\")\n",
    "    df = df.copy()\n",
    "\n",
    "    initial_rows = len(df)\n",
    "\n",
    "    # Handle missing values\n",
    "    for column, strategy in cleaning_config.get(\"missing_values\", {}).items():\n",
    "        if df[column].isnull().any():\n",
    "            if strategy == \"median\":\n",
    "                fill_value = df[column].median()\n",
    "                df[column] = df[column].fillna(fill_value)\n",
    "                logger.info(f\"Filled {column} with median: {fill_value}\")\n",
    "            elif strategy == \"mean\":\n",
    "                fill_value = df[column].mean()\n",
    "                df[column] = df[column].fillna(fill_value)\n",
    "                logger.info(f\"Filled {column} with mean: {fill_value}\")\n",
    "\n",
    "    # Remove duplicates\n",
    "    if cleaning_config.get(\"remove_duplicates\", False):\n",
    "        before = len(df)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        logger.info(f\"Removed {before - len(df)} duplicate rows\")\n",
    "\n",
    "    # Convert data types\n",
    "    df['is_active'] = df['is_active'].astype(bool)\n",
    "    df['signup_date'] = pd.to_datetime(df['signup_date'])\n",
    "    df['last_login'] = pd.to_datetime(df['last_login'])\n",
    "\n",
    "    logger.info(f\"Cleaning complete: {initial_rows} ‚Üí {len(df)} rows\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91152e4e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üß± STEP 8 ‚Äî Orchestration (Pipeline Brain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83efa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from loguru import logger\n",
    "from src.ingestion.loader import load_data\n",
    "from src.validation.validator import validate_schema\n",
    "from src.cleaning.cleaner import clean_data\n",
    "\n",
    "def run_pipeline(input_path, schema_path, pipeline_config_path, output_path):\n",
    "    \"\"\"Execute complete preprocessing pipeline.\"\"\"\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"STARTING DATA PREPROCESSING PIPELINE\")\n",
    "    logger.info(\"=\" * 60)\n",
    "\n",
    "    # Load config\n",
    "    with open(pipeline_config_path) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    # STEP 1: Ingestion\n",
    "    if config[\"steps\"][\"ingestion\"]:\n",
    "        df = load_data(input_path)\n",
    "    else:\n",
    "        logger.warning(\"Ingestion skipped\")\n",
    "        return\n",
    "\n",
    "    # STEP 2: Validation\n",
    "    if config[\"steps\"][\"validation\"]:\n",
    "        report = validate_schema(df, schema_path)\n",
    "        if report[\"errors\"] and not any(\"warnings\" in e for e in report[\"errors\"]):\n",
    "            logger.error(f\"Validation failed with errors: {report['errors']}\")\n",
    "            raise Exception(f\"Pipeline halted due to validation errors\")\n",
    "        logger.info(f\"Validation passed: {report['warnings']}\")\n",
    "    \n",
    "    # STEP 3: Cleaning\n",
    "    if config[\"steps\"][\"cleaning\"]:\n",
    "        df = clean_data(df, config.get(\"cleaning\", {}))\n",
    "\n",
    "    # STEP 4: Save\n",
    "    df.to_csv(output_path, index=False)\n",
    "    logger.info(f\"Cleaned data saved to: {output_path}\")\n",
    "\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9097dd25",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üß± STEP 9 ‚Äî Main Entry Point\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ebd2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.orchestration.pipeline import run_pipeline\n",
    "from loguru import logger\n",
    "import sys\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.remove()\n",
    "    logger.add(sys.stdout, format=\"<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | {message}\")\n",
    "    logger.add(\"logs/pipeline.log\", format=\"{time} | {level: <8} | {message}\")\n",
    "\n",
    "    try:\n",
    "        report = run_pipeline(\n",
    "            input_path=\"data/raw/user_activity_data.csv\",\n",
    "            schema_path=\"configs/schema.yaml\",\n",
    "            pipeline_config_path=\"configs/pipeline.yaml\",\n",
    "            output_path=\"data/processed/clean_data.csv\"\n",
    "        )\n",
    "\n",
    "        print(\"\\n‚úÖ Pipeline Status:\")\n",
    "        print(f\"   Warnings: {len(report['warnings'])}\")\n",
    "        print(f\"   Errors: {len(report['errors'])}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bcec4b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üß± STEP 10 ‚Äî Create `__init__.py` files\n",
    "\n",
    "Create empty files to make modules importable:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466cfc70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5809a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c910b43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df1e310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68fb94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30ddf6f8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ STEP 11 ‚Äî Run the Pipeline\n",
    "\n",
    "In Terminal:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3268a4f7",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "cd /Users/ankitkumar/Desktop/all_code_of_collage/learning/datapipline\n",
    "python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b94d65",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Expected output:**\n",
    "- ‚úÖ Data loaded\n",
    "- ‚úÖ Schema validated\n",
    "- ‚úÖ Missing values filled (age ‚Üí median)\n",
    "- ‚úÖ Clean CSV saved to `data/processed/clean_data.csv`\n",
    "- üìä Log file created in `logs/pipeline.log`\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What You've Built\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|---------|\n",
    "| **Loader** | Safe CSV ingestion with validation |\n",
    "| **Validator** | Schema enforcement (nullable, range, category checks) |\n",
    "| **Cleaner** | Missing value imputation, deduplication, type conversion |\n",
    "| **Orchestrator** | Coordinated pipeline execution |\n",
    "| **Config-driven** | No hardcoding ‚Äî behavior controlled via YAML |\n",
    "\n",
    "This is **production-ready** ‚Äî used by real data teams. üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
